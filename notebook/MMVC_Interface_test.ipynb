{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7-O9IRzogIx"
      },
      "source": [
        "# モデルの精度を確認するためのインターフェース\n",
        "\n",
        "ver.2022/04/26"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK2UHlpmoyLW"
      },
      "source": [
        "## 1 概要\n",
        "「Train_MMVC.ipynb」で学習したモデルでTTSと非リアルタイムのVCを行い、モデルの精度を検証します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1222,
          "status": "ok",
          "timestamp": 1653968669186,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "yaNipgu-enJo",
        "outputId": "5a167401-052c-4251-c9da-b7722666cb20"
      },
      "outputs": [],
      "source": [
        "%cd ../\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5641,
          "status": "ok",
          "timestamp": 1653907247349,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "P3QYLvY4e38A"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import commons\n",
        "import utils\n",
        "#from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
        "#from models import SynthesizerTrn\n",
        "from text.symbols import symbols\n",
        "from text import text_to_sequence\n",
        "\n",
        "from scipy.io.wavfile import write\n",
        "import pyopenjtalk\n",
        "\n",
        "def get_text(text, hps):\n",
        "    text_norm = text_to_sequence(text, hps.data.text_cleaners)\n",
        "    if hps.data.add_blank:\n",
        "        text_norm = commons.intersperse(text_norm, 0)\n",
        "    text_norm = torch.LongTensor(text_norm)\n",
        "    return text_norm\n",
        "\n",
        "def mozi2phone(mozi):\n",
        "    text = pyopenjtalk.g2p(mozi)\n",
        "    text = \"sil \" + text + \" sil\"\n",
        "    text = text.replace(' ', '-')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9FaQR4F5hmJ"
      },
      "source": [
        "models.py そのものをコピーしてここで修正して試す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 5679,
          "status": "ok",
          "timestamp": 1653907707216,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "r1iBMm_33g_r",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import commons\n",
        "import modules\n",
        "import attentions\n",
        "import monotonic_align\n",
        "\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "from commons import init_weights, get_padding\n",
        "\n",
        "\n",
        "class StochasticDurationPredictor(nn.Module):\n",
        "  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, n_flows=4, gin_channels=0):\n",
        "    super().__init__()\n",
        "    filter_channels = in_channels # it needs to be removed from future version.\n",
        "    self.in_channels = in_channels\n",
        "    self.filter_channels = filter_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.p_dropout = p_dropout\n",
        "    self.n_flows = n_flows\n",
        "    self.gin_channels = gin_channels\n",
        "\n",
        "    self.log_flow = modules.Log()\n",
        "    self.flows = nn.ModuleList()\n",
        "    self.flows.append(modules.ElementwiseAffine(2))\n",
        "    for i in range(n_flows):\n",
        "      self.flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n",
        "      self.flows.append(modules.Flip())\n",
        "\n",
        "    self.post_pre = nn.Conv1d(1, filter_channels, 1)\n",
        "    self.post_proj = nn.Conv1d(filter_channels, filter_channels, 1)\n",
        "    self.post_convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n",
        "    self.post_flows = nn.ModuleList()\n",
        "    self.post_flows.append(modules.ElementwiseAffine(2))\n",
        "    for i in range(4):\n",
        "      self.post_flows.append(modules.ConvFlow(2, filter_channels, kernel_size, n_layers=3))\n",
        "      self.post_flows.append(modules.Flip())\n",
        "\n",
        "    self.pre = nn.Conv1d(in_channels, filter_channels, 1)\n",
        "    self.proj = nn.Conv1d(filter_channels, filter_channels, 1)\n",
        "    self.convs = modules.DDSConv(filter_channels, kernel_size, n_layers=3, p_dropout=p_dropout)\n",
        "    if gin_channels != 0:\n",
        "      self.cond = nn.Conv1d(gin_channels, filter_channels, 1)\n",
        "\n",
        "  def forward(self, x, x_mask, w=None, g=None, reverse=False, noise_scale=1.0):\n",
        "    x = torch.detach(x)\n",
        "    x = self.pre(x)\n",
        "    if g is not None:\n",
        "      g = torch.detach(g)\n",
        "      x = x + self.cond(g)\n",
        "    x = self.convs(x, x_mask)\n",
        "    x = self.proj(x) * x_mask\n",
        "\n",
        "    if not reverse:\n",
        "      flows = self.flows\n",
        "      assert w is not None\n",
        "\n",
        "      logdet_tot_q = 0 \n",
        "      h_w = self.post_pre(w)\n",
        "      h_w = self.post_convs(h_w, x_mask)\n",
        "      h_w = self.post_proj(h_w) * x_mask\n",
        "      e_q = torch.randn(w.size(0), 2, w.size(2)).to(device=x.device, dtype=x.dtype) * x_mask\n",
        "      z_q = e_q\n",
        "      for flow in self.post_flows:\n",
        "        z_q, logdet_q = flow(z_q, x_mask, g=(x + h_w))\n",
        "        logdet_tot_q += logdet_q\n",
        "      z_u, z1 = torch.split(z_q, [1, 1], 1) \n",
        "      u = torch.sigmoid(z_u) * x_mask\n",
        "      z0 = (w - u) * x_mask\n",
        "      logdet_tot_q += torch.sum((F.logsigmoid(z_u) + F.logsigmoid(-z_u)) * x_mask, [1,2])\n",
        "      logq = torch.sum(-0.5 * (math.log(2*math.pi) + (e_q**2)) * x_mask, [1,2]) - logdet_tot_q\n",
        "\n",
        "      logdet_tot = 0\n",
        "      z0, logdet = self.log_flow(z0, x_mask)\n",
        "      logdet_tot += logdet\n",
        "      z = torch.cat([z0, z1], 1)\n",
        "      for flow in flows:\n",
        "        z, logdet = flow(z, x_mask, g=x, reverse=reverse)\n",
        "        logdet_tot = logdet_tot + logdet\n",
        "      nll = torch.sum(0.5 * (math.log(2*math.pi) + (z**2)) * x_mask, [1,2]) - logdet_tot\n",
        "      return nll + logq # [b]\n",
        "    else:\n",
        "      flows = list(reversed(self.flows))\n",
        "      flows = flows[:-2] + [flows[-1]] # remove a useless vflow\n",
        "      z = torch.randn(x.size(0), 2, x.size(2)).to(device=x.device, dtype=x.dtype) * noise_scale\n",
        "      for flow in flows:\n",
        "        z = flow(z, x_mask, g=x, reverse=reverse)\n",
        "      z0, z1 = torch.split(z, [1, 1], 1)\n",
        "      logw = z0\n",
        "      return logw\n",
        "\n",
        "\n",
        "class DurationPredictor(nn.Module):\n",
        "  def __init__(self, in_channels, filter_channels, kernel_size, p_dropout, gin_channels=0):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.filter_channels = filter_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.p_dropout = p_dropout\n",
        "    self.gin_channels = gin_channels\n",
        "\n",
        "    self.drop = nn.Dropout(p_dropout)\n",
        "    self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
        "    self.norm_1 = modules.LayerNorm(filter_channels)\n",
        "    self.conv_2 = nn.Conv1d(filter_channels, filter_channels, kernel_size, padding=kernel_size//2)\n",
        "    self.norm_2 = modules.LayerNorm(filter_channels)\n",
        "    self.proj = nn.Conv1d(filter_channels, 1, 1)\n",
        "\n",
        "    if gin_channels != 0:\n",
        "      self.cond = nn.Conv1d(gin_channels, in_channels, 1)\n",
        "\n",
        "  def forward(self, x, x_mask, g=None):\n",
        "    x = torch.detach(x)\n",
        "    if g is not None:\n",
        "      g = torch.detach(g)\n",
        "      x = x + self.cond(g)\n",
        "    x = self.conv_1(x * x_mask)\n",
        "    x = torch.relu(x)\n",
        "    x = self.norm_1(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.conv_2(x * x_mask)\n",
        "    x = torch.relu(x)\n",
        "    x = self.norm_2(x)\n",
        "    x = self.drop(x)\n",
        "    x = self.proj(x * x_mask)\n",
        "    return x * x_mask\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "  def __init__(self,\n",
        "      n_vocab,\n",
        "      out_channels,\n",
        "      hidden_channels,\n",
        "      filter_channels,\n",
        "      n_heads,\n",
        "      n_layers,\n",
        "      kernel_size,\n",
        "      p_dropout):\n",
        "    super().__init__()\n",
        "    self.n_vocab = n_vocab\n",
        "    self.out_channels = out_channels\n",
        "    self.hidden_channels = hidden_channels\n",
        "    self.filter_channels = filter_channels\n",
        "    self.n_heads = n_heads\n",
        "    self.n_layers = n_layers\n",
        "    self.kernel_size = kernel_size\n",
        "    self.p_dropout = p_dropout\n",
        "\n",
        "    self.emb = nn.Embedding(n_vocab, hidden_channels)\n",
        "    nn.init.normal_(self.emb.weight, 0.0, hidden_channels**-0.5)\n",
        "\n",
        "    self.encoder = attentions.Encoder(\n",
        "      hidden_channels,\n",
        "      filter_channels,\n",
        "      n_heads,\n",
        "      n_layers,\n",
        "      kernel_size,\n",
        "      p_dropout)\n",
        "    self.proj= nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
        "\n",
        "  def forward(self, x, x_lengths):\n",
        "    x = self.emb(x) * math.sqrt(self.hidden_channels) # [b, t, h]\n",
        "    x = torch.transpose(x, 1, -1) # [b, h, t]\n",
        "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
        "\n",
        "    x = self.encoder(x * x_mask, x_mask)\n",
        "    stats = self.proj(x) * x_mask\n",
        "\n",
        "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
        "    return x, m, logs, x_mask\n",
        "\n",
        "\n",
        "class ResidualCouplingBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "      channels,\n",
        "      hidden_channels,\n",
        "      kernel_size,\n",
        "      dilation_rate,\n",
        "      n_layers,\n",
        "      n_flows=4,\n",
        "      gin_channels=0):\n",
        "    super().__init__()\n",
        "    self.channels = channels\n",
        "    self.hidden_channels = hidden_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.dilation_rate = dilation_rate\n",
        "    self.n_layers = n_layers\n",
        "    self.n_flows = n_flows\n",
        "    self.gin_channels = gin_channels\n",
        "\n",
        "    self.flows = nn.ModuleList()\n",
        "    for i in range(n_flows):\n",
        "      self.flows.append(modules.ResidualCouplingLayer(channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels, mean_only=True))\n",
        "      self.flows.append(modules.Flip())\n",
        "\n",
        "  def forward(self, x, x_mask, g=None, reverse=False):\n",
        "    if not reverse:\n",
        "      for flow in self.flows:\n",
        "        x, _ = flow(x, x_mask, g=g, reverse=reverse)\n",
        "    else:\n",
        "      for flow in reversed(self.flows):\n",
        "        x = flow(x, x_mask, g=g, reverse=reverse)\n",
        "    return x\n",
        "\n",
        "\n",
        "class PosteriorEncoder(nn.Module):\n",
        "  def __init__(self,\n",
        "      in_channels,\n",
        "      out_channels,\n",
        "      hidden_channels,\n",
        "      kernel_size,\n",
        "      dilation_rate,\n",
        "      n_layers,\n",
        "      gin_channels=0):\n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.hidden_channels = hidden_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.dilation_rate = dilation_rate\n",
        "    self.n_layers = n_layers\n",
        "    self.gin_channels = gin_channels\n",
        "\n",
        "    self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n",
        "    self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)\n",
        "    self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n",
        "\n",
        "  def forward(self, x, x_lengths, g=None):\n",
        "    x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
        "    x = self.pre(x) * x_mask\n",
        "    x = self.enc(x, x_mask, g=g)\n",
        "    stats = self.proj(x) * x_mask\n",
        "    m, logs = torch.split(stats, self.out_channels, dim=1)\n",
        "    z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n",
        "    return z, m, logs, x_mask\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, initial_channel, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):\n",
        "        super(Generator, self).__init__()\n",
        "        self.num_kernels = len(resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(upsample_rates)\n",
        "        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)\n",
        "        resblock = modules.ResBlock1 if resblock == '1' else modules.ResBlock2\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
        "            self.ups.append(weight_norm(\n",
        "                ConvTranspose1d(upsample_initial_channel//(2**i), upsample_initial_channel//(2**(i+1)),\n",
        "                                k, u, padding=(k-u)//2)))\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for i in range(len(self.ups)):\n",
        "            ch = upsample_initial_channel//(2**(i+1))\n",
        "            for j, (k, d) in enumerate(zip(resblock_kernel_sizes, resblock_dilation_sizes)):\n",
        "                self.resblocks.append(resblock(ch, k, d))\n",
        "\n",
        "        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n",
        "        self.ups.apply(init_weights)\n",
        "\n",
        "        if gin_channels != 0:\n",
        "            #self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n",
        "            gin_channels = 0\n",
        "\n",
        "    def forward(self, x, g=None):\n",
        "        x = self.conv_pre(x)\n",
        "        if g is not None:\n",
        "          #x = x + self.cond(g)\n",
        "          g=None\n",
        "\n",
        "        for i in range(self.num_upsamples):\n",
        "            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n",
        "            x = self.ups[i](x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for l in self.ups:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.resblocks:\n",
        "            l.remove_weight_norm()\n",
        "\n",
        "\n",
        "class DiscriminatorP(torch.nn.Module):\n",
        "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
        "        super(DiscriminatorP, self).__init__()\n",
        "        self.period = period\n",
        "        self.use_spectral_norm = use_spectral_norm\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
        "            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
        "            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
        "            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n",
        "            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(get_padding(kernel_size, 1), 0))),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        # 1d to 2d\n",
        "        b, c, t = x.shape\n",
        "        if t % self.period != 0: # pad first\n",
        "            n_pad = self.period - (t % self.period)\n",
        "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
        "            t = t + n_pad\n",
        "        x = x.view(b, c, t // self.period, self.period)\n",
        "\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class DiscriminatorS(torch.nn.Module):\n",
        "    def __init__(self, use_spectral_norm=False):\n",
        "        super(DiscriminatorS, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n",
        "            norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n",
        "            norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiPeriodDiscriminator(torch.nn.Module):\n",
        "    def __init__(self, use_spectral_norm=False):\n",
        "        super(MultiPeriodDiscriminator, self).__init__()\n",
        "        periods = [2,3,5,7,11]\n",
        "\n",
        "        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n",
        "        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\n",
        "        self.discriminators = nn.ModuleList(discs)\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "\n",
        "\n",
        "class SynthesizerTrn(nn.Module):\n",
        "  \"\"\"\n",
        "  Synthesizer for Training\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, \n",
        "    n_vocab,\n",
        "    spec_channels,\n",
        "    segment_size,\n",
        "    inter_channels,\n",
        "    hidden_channels,\n",
        "    filter_channels,\n",
        "    n_heads,\n",
        "    n_layers,\n",
        "    kernel_size,\n",
        "    p_dropout,\n",
        "    resblock, \n",
        "    resblock_kernel_sizes, \n",
        "    resblock_dilation_sizes, \n",
        "    upsample_rates, \n",
        "    upsample_initial_channel, \n",
        "    upsample_kernel_sizes,\n",
        "    n_speakers=0,\n",
        "    gin_channels=0,\n",
        "    use_sdp=True,\n",
        "    **kwargs):\n",
        "\n",
        "    super().__init__()\n",
        "    self.n_vocab = n_vocab\n",
        "    self.spec_channels = spec_channels\n",
        "    self.hidden_channels = hidden_channels\n",
        "    self.filter_channels = filter_channels\n",
        "    self.n_heads = n_heads\n",
        "    self.n_layers = n_layers\n",
        "    self.kernel_size = kernel_size\n",
        "    self.p_dropout = p_dropout\n",
        "    self.resblock = resblock\n",
        "    self.resblock_kernel_sizes = resblock_kernel_sizes\n",
        "    self.resblock_dilation_sizes = resblock_dilation_sizes\n",
        "    self.upsample_rates = upsample_rates\n",
        "    self.upsample_initial_channel = upsample_initial_channel\n",
        "    self.upsample_kernel_sizes = upsample_kernel_sizes\n",
        "    self.segment_size = segment_size\n",
        "    self.n_speakers = n_speakers\n",
        "    self.gin_channels = gin_channels\n",
        "\n",
        "    self.use_sdp = use_sdp\n",
        "\n",
        "    self.enc_p = TextEncoder(n_vocab,\n",
        "        inter_channels,\n",
        "        hidden_channels,\n",
        "        filter_channels,\n",
        "        n_heads,\n",
        "        n_layers,\n",
        "        kernel_size,\n",
        "        p_dropout)\n",
        "    self.dec = Generator(inter_channels, resblock, resblock_kernel_sizes, resblock_dilation_sizes, upsample_rates, upsample_initial_channel, upsample_kernel_sizes, gin_channels=gin_channels)\n",
        "    self.enc_q = PosteriorEncoder(spec_channels, inter_channels, hidden_channels, 5, 1, 16, gin_channels=gin_channels)\n",
        "    self.flow = ResidualCouplingBlock(inter_channels, hidden_channels, 5, 1, 4, gin_channels=gin_channels)\n",
        "\n",
        "    if use_sdp:\n",
        "      self.dp = StochasticDurationPredictor(hidden_channels, 192, 3, 0.5, 4, gin_channels=gin_channels)\n",
        "    else:\n",
        "      self.dp = DurationPredictor(hidden_channels, 256, 3, 0.5, gin_channels=gin_channels)\n",
        "\n",
        "    if n_speakers > 1:\n",
        "      self.emb_g = nn.Embedding(n_speakers, gin_channels)\n",
        "\n",
        "  def forward(self, x, x_lengths, y, y_lengths, sid=None):\n",
        "\n",
        "    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
        "    if self.n_speakers > 0:\n",
        "      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
        "    else:\n",
        "      g = None\n",
        "\n",
        "    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n",
        "    z_p = self.flow(z, y_mask, g=g)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # negative cross-entropy\n",
        "      s_p_sq_r = torch.exp(-2 * logs_p) # [b, d, t]\n",
        "      neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True) # [b, 1, t_s]\n",
        "      neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
        "      neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) # [b, t_t, d] x [b, d, t_s] = [b, t_t, t_s]\n",
        "      neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True) # [b, 1, t_s]\n",
        "      neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4\n",
        "\n",
        "      attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
        "      attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
        "\n",
        "    w = attn.sum(2)\n",
        "    if self.use_sdp:\n",
        "      l_length = self.dp(x, x_mask, w, g=g)\n",
        "      l_length = l_length / torch.sum(x_mask)\n",
        "    else:\n",
        "      logw_ = torch.log(w + 1e-6) * x_mask\n",
        "      logw = self.dp(x, x_mask, g=g)\n",
        "      l_length = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask) # for averaging \n",
        "\n",
        "    # expand prior\n",
        "    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)\n",
        "    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "    z_slice, ids_slice = commons.rand_slice_segments(z, y_lengths, self.segment_size)\n",
        "    o = self.dec(z_slice, g=g)\n",
        "    return o, l_length, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n",
        "\n",
        "  def infer(self, x, x_lengths, sid=None, noise_scale=1, length_scale=1, noise_scale_w=1., max_len=None):\n",
        "    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)\n",
        "    if self.n_speakers > 0:\n",
        "      g = self.emb_g(sid).unsqueeze(-1) # [b, h, 1]\n",
        "    else:\n",
        "      g = None\n",
        "\n",
        "    if self.use_sdp:\n",
        "      logw = self.dp(x, x_mask, g=g, reverse=True, noise_scale=noise_scale_w)\n",
        "    else:\n",
        "      logw = self.dp(x, x_mask, g=g)\n",
        "    w = torch.exp(logw) * x_mask * length_scale\n",
        "    w_ceil = torch.ceil(w)\n",
        "    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
        "    y_mask = torch.unsqueeze(commons.sequence_mask(y_lengths, None), 1).to(x_mask.dtype)\n",
        "    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)\n",
        "    attn = commons.generate_path(w_ceil, attn_mask)\n",
        "\n",
        "    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
        "    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
        "\n",
        "    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p) * noise_scale\n",
        "    z = self.flow(z_p, y_mask, g=g, reverse=True)\n",
        "    o = self.dec((z * y_mask)[:,:,:max_len], g=g)\n",
        "    return o, attn, y_mask, (z, z_p, m_p, logs_p)\n",
        "\n",
        "  def voice_conversion(self, y, y_lengths, sid_src, sid_tgt):\n",
        "    print(f\"(SynthesizerTrn::voice_conversion) y.shape:{y.shape}, y_lengths:{y_lengths}\")\n",
        "    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
        "    g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
        "    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
        "    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
        "    print(f\"(SynthesizerTrn::voice_conversion) z.shape:{z.shape}\")\n",
        "    # 4bitに量子化\n",
        "    z4 = z\n",
        "    z4 = torch.round(torch.clamp(z4, max=7, min=-8) * 1.0)\n",
        "    # 一番情報量ありそうな行を削除\n",
        "    #print(f\"delete 33\")\n",
        "    #z4[0][33] = 0.0\n",
        "    # なんかの音素\n",
        "    #print(f\"delete 139\")\n",
        "    #z4[0][139] = 0.0\n",
        "    #print(f\"delete 000-50\")\n",
        "    #z4[0][0:51] = 0.0\n",
        "    # サンプリングタイミングの偶数番号を削除\n",
        "    #z02 = z4.squeeze(0).t()\n",
        "    #z02[0::2]=0.0\n",
        "    #z4 = z02.t().unsqueeze(0)\n",
        "    # サンプリングタイミングの奇数番号を偶数番号からコピー\n",
        "    #z02 = z1.squeeze(0).t()\n",
        "    # 2倍に薄める\n",
        "    #for i in range(len(z02) // 2):\n",
        "    #  z02[i*2+1] = z02[i*2]\n",
        "    # 4倍に薄める\n",
        "    #for i in range(len(z02) // 4):\n",
        "    #  z02[i*4+1] = z02[i*4]\n",
        "    #  z02[i*4+2] = z02[i*4]\n",
        "    #  z02[i*4+3] = z02[i*4]\n",
        "    #z4 = z02.t().unsqueeze(0)\n",
        "    #z_p = self.flow(z, y_mask, g=g_src)\n",
        "    z_p = self.flow(z4, y_mask, g=g_src)\n",
        "    print(f\"(SynthesizerTrn::voice_conversion) z_p.shape:{z_p.shape}\")\n",
        "    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
        "    print(f\"(SynthesizerTrn::voice_conversion) z_hat.shape:{z_hat.shape}\")\n",
        "    o_hat = self.dec(z_hat * y_mask, g=g_tgt)\n",
        "    print(f\"(SynthesizerTrn::voice_conversion) o_hat.shape:{o_hat.shape}\")\n",
        "    #print(f\"test: {type(o_hat)}\")\n",
        "    #return o_hat, y_mask, (z, z_p, z_hat)\n",
        "    return o_hat, y_mask, (z4, z_p, z_hat)\n",
        "\n",
        "  def voice_ra_pa_db(self, y, y_lengths, sid_src, sid_tgt):\n",
        "    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
        "    g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
        "    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
        "    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
        "    o_hat = self.dec(z * y_mask, g=g_tgt)\n",
        "    return o_hat, y_mask, (z)\n",
        "\n",
        "  def voice_ra_pa_da(self, y, y_lengths, sid_src, sid_tgt):\n",
        "    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
        "    g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
        "    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
        "    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
        "    o_hat = self.dec(z * y_mask, g=g_src)\n",
        "    return o_hat, y_mask, (z)\n",
        "\n",
        "  def voice_conversion_cycle(self, y, y_lengths, sid_src, sid_tgt):\n",
        "    assert self.n_speakers > 0, \"n_speakers have to be larger than 0.\"\n",
        "    g_src = self.emb_g(sid_src).unsqueeze(-1)\n",
        "    g_tgt = self.emb_g(sid_tgt).unsqueeze(-1)\n",
        "    z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g_src)\n",
        "    z_p = self.flow(z, y_mask, g=g_src)\n",
        "    z_hat = self.flow(z_p, y_mask, g=g_tgt, reverse=True)\n",
        "    z_p_hat = self.flow(z_hat, y_mask, g=g_tgt)\n",
        "    z_hat_hat = self.flow(z_p_hat, y_mask, g=g_src, reverse=True)\n",
        "    o_hat = self.dec(z_hat_hat * y_mask, g=g_tgt)\n",
        "    return o_hat, y_mask, (z, z_p, z_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2298,
          "status": "ok",
          "timestamp": 1653907249645,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "0zCU7wrzxRik",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import tqdm\n",
        "\n",
        "import commons \n",
        "#from mel_processing import spectrogram_torch\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence, cleaned_text_to_sequence\n",
        "\n",
        "#add\n",
        "from retry import retry\n",
        "\n",
        "\n",
        "class TextAudioLoader(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        1) loads audio, text pairs\n",
        "        2) normalizes text and converts them to sequences of integers\n",
        "        3) computes spectrograms from audio files.\n",
        "    \"\"\"\n",
        "    def __init__(self, audiopaths_and_text, hparams, use_test = True):\n",
        "        self.audiopaths_and_text = load_filepaths_and_text(audiopaths_and_text)\n",
        "        self.text_cleaners  = hparams.text_cleaners\n",
        "        self.max_wav_value  = hparams.max_wav_value\n",
        "        self.sampling_rate  = hparams.sampling_rate\n",
        "        self.filter_length  = hparams.filter_length \n",
        "        self.hop_length     = hparams.hop_length \n",
        "        self.win_length     = hparams.win_length\n",
        "        self.sampling_rate  = hparams.sampling_rate\n",
        "        self.use_test  = use_test\n",
        "\n",
        "        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
        "\n",
        "        self.add_blank = hparams.add_blank\n",
        "        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
        "        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
        "\n",
        "        random.seed(1234)\n",
        "        random.shuffle(self.audiopaths_and_text)\n",
        "        self._filter()\n",
        "\n",
        "\n",
        "    def _filter(self):\n",
        "        \"\"\"\n",
        "        Filter text & store spec lengths\n",
        "        \"\"\"\n",
        "        # Store spectrogram lengths for Bucketing\n",
        "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
        "        # spec_length = wav_length // hop_length\n",
        "\n",
        "        audiopaths_and_text_new = []\n",
        "        lengths = []\n",
        "        for audiopath, text in self.audiopaths_and_text:\n",
        "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
        "                audiopaths_and_text_new.append([audiopath, text])\n",
        "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
        "        self.audiopaths_and_text = audiopaths_and_text_new\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def get_audio_text_pair(self, audiopath_and_text):\n",
        "        # separate filename and text\n",
        "        audiopath, text = audiopath_and_text[0], audiopath_and_text[1]\n",
        "        text = self.get_text(text)\n",
        "        if self.use_test != True:\n",
        "          text = torch.as_tensor(\"a\")\n",
        "        spec, wav = self.get_audio(audiopath)\n",
        "        return (text, spec, wav)\n",
        "\n",
        "    def get_audio(self, filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != self.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
        "                sampling_rate, self.sampling_rate))\n",
        "        audio_norm = audio / self.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n",
        "        if os.path.exists(spec_filename):\n",
        "            spec = torch.load(spec_filename)\n",
        "        else:\n",
        "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
        "                self.sampling_rate, self.hop_length, self.win_length,\n",
        "                center=False)\n",
        "            spec = torch.squeeze(spec, 0)\n",
        "            torch.save(spec, spec_filename)\n",
        "        return spec, audio_norm\n",
        "\n",
        "    def get_text(self, text):\n",
        "        if self.cleaned_text:\n",
        "            text_norm = cleaned_text_to_sequence(text)\n",
        "        else:\n",
        "            text_norm = text_to_sequence(text, self.text_cleaners)\n",
        "        if self.add_blank:\n",
        "            text_norm = commons.intersperse(text_norm, 0)\n",
        "        text_norm = torch.LongTensor(text_norm)\n",
        "        return text_norm\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_audio_text_pair(self.audiopaths_and_text[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiopaths_and_text)\n",
        "\n",
        "\n",
        "class TextAudioCollate():\n",
        "    \"\"\" Zero-pads model inputs and targets\n",
        "    \"\"\"\n",
        "    def __init__(self, return_ids=False):\n",
        "        self.return_ids = return_ids\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Collate's training batch from normalized text and aduio\n",
        "        PARAMS\n",
        "        ------\n",
        "        batch: [text_normalized, spec_normalized, wav_normalized]\n",
        "        \"\"\"\n",
        "        # Right zero-pad all one-hot text sequences to max input length\n",
        "        _, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([x[1].size(1) for x in batch]),\n",
        "            dim=0, descending=True)\n",
        "\n",
        "        max_text_len = max([len(x[0]) for x in batch])\n",
        "        max_spec_len = max([x[1].size(1) for x in batch])\n",
        "        max_wav_len = max([x[2].size(1) for x in batch])\n",
        "\n",
        "        text_lengths = torch.LongTensor(len(batch))\n",
        "        spec_lengths = torch.LongTensor(len(batch))\n",
        "        wav_lengths = torch.LongTensor(len(batch))\n",
        "\n",
        "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
        "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
        "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
        "        text_padded.zero_()\n",
        "        spec_padded.zero_()\n",
        "        wav_padded.zero_()\n",
        "        for i in range(len(ids_sorted_decreasing)):\n",
        "            row = batch[ids_sorted_decreasing[i]]\n",
        "\n",
        "            text = row[0]\n",
        "            text_padded[i, :text.size(0)] = text\n",
        "            text_lengths[i] = text.size(0)\n",
        "\n",
        "            spec = row[1]\n",
        "            spec_padded[i, :, :spec.size(1)] = spec\n",
        "            spec_lengths[i] = spec.size(1)\n",
        "\n",
        "            wav = row[2]\n",
        "            wav_padded[i, :, :wav.size(1)] = wav\n",
        "            wav_lengths[i] = wav.size(1)\n",
        "\n",
        "        if self.return_ids:\n",
        "            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, ids_sorted_decreasing\n",
        "        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths\n",
        "\n",
        "\n",
        "\"\"\"Multi speaker version\"\"\"\n",
        "class TextAudioSpeakerLoader(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "        1) loads audio, speaker_id, text pairs\n",
        "        2) normalizes text and converts them to sequences of integers\n",
        "        3) computes spectrograms from audio files.\n",
        "    \"\"\"\n",
        "    def __init__(self, audiopaths_sid_text, hparams, no_text = False):\n",
        "        self.audiopaths_sid_text = load_filepaths_and_text(audiopaths_sid_text)\n",
        "        self.text_cleaners = hparams.text_cleaners\n",
        "        self.max_wav_value = hparams.max_wav_value\n",
        "        self.sampling_rate = hparams.sampling_rate\n",
        "        self.filter_length  = hparams.filter_length\n",
        "        self.hop_length     = hparams.hop_length\n",
        "        self.win_length     = hparams.win_length\n",
        "        self.sampling_rate  = hparams.sampling_rate\n",
        "        self.no_text = no_text\n",
        "\n",
        "        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
        "\n",
        "        self.add_blank = hparams.add_blank\n",
        "        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
        "        self.max_text_len = getattr(hparams, \"max_text_len\", 1000)\n",
        "\n",
        "        random.seed(1234)\n",
        "        random.shuffle(self.audiopaths_sid_text)\n",
        "        self._filter()\n",
        "\n",
        "    @retry(exceptions=(IOError), tries=10, delay=1)\n",
        "    def _filter(self):\n",
        "        \"\"\"\n",
        "        Filter text & store spec lengths\n",
        "        \"\"\"\n",
        "        # Store spectrogram lengths for Bucketing\n",
        "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
        "        # spec_length = wav_length // hop_length\n",
        "\n",
        "        audiopaths_sid_text_new = []\n",
        "        lengths = []\n",
        "        \n",
        "        for audiopath, sid, text in tqdm.tqdm(self.audiopaths_sid_text):\n",
        "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
        "                audiopaths_sid_text_new.append([audiopath, sid, text])\n",
        "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
        "        self.audiopaths_sid_text = audiopaths_sid_text_new\n",
        "        self.lengths = lengths\n",
        "\n",
        "    def get_audio_text_speaker_pair(self, audiopath_sid_text):\n",
        "        # separate filename, speaker_id and text\n",
        "        audiopath, sid, text = audiopath_sid_text[0], audiopath_sid_text[1], audiopath_sid_text[2]\n",
        "        text = self.get_text(text)\n",
        "        if self.no_text:\n",
        "          text = self.get_text(\"a\")\n",
        "        spec, wav = self.get_audio(audiopath)\n",
        "        sid = self.get_sid(sid)\n",
        "        return (text, spec, wav, sid)\n",
        "        \n",
        "    @retry(exceptions=(PermissionError), tries=100, delay=10)\n",
        "    def get_audio(self, filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        # print(sampling_rate)\n",
        "        # print(self.sampling_rate)\n",
        "        # print(filename)\n",
        "        print(\"--- get_audio ---\")\n",
        "        try:\n",
        "            if sampling_rate != self.sampling_rate:\n",
        "                raise ValueError(\"[Error] Exception: source {} SR doesn't match target {} SR\".format(\n",
        "                    sampling_rate, self.sampling_rate))\n",
        "        except ValueError as e:\n",
        "            print(e)\n",
        "            exit()\n",
        "        audio_norm = audio / self.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n",
        "        spec_file_path =os.path.dirname(spec_filename) + \"/spec/\"+ os.path.basename(spec_filename)\n",
        "        if os.path.exists(spec_file_path):\n",
        "            #spec = torch.load(spec_file_path)\n",
        "            if os.path.isdir(os.path.dirname(spec_filename) + \"/spec\") == False:\n",
        "              os.mkdir(os.path.dirname(spec_filename) + \"/spec\")\n",
        "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
        "                self.sampling_rate, self.hop_length, self.win_length,\n",
        "                center=False)\n",
        "            spec = torch.squeeze(spec, 0)\n",
        "            torch.save(spec, spec_file_path)\n",
        "        else:\n",
        "            if os.path.isdir(os.path.dirname(spec_filename) + \"/spec\") == False:\n",
        "              os.mkdir(os.path.dirname(spec_filename) + \"/spec\")\n",
        "            spec = spectrogram_torch(audio_norm, self.filter_length,\n",
        "                self.sampling_rate, self.hop_length, self.win_length,\n",
        "                center=False)\n",
        "            spec = torch.squeeze(spec, 0)\n",
        "            torch.save(spec, spec_file_path)\n",
        "        # spec = spectrogram_torch(audio_norm, self.filter_length,\n",
        "        #     self.sampling_rate, self.hop_length, self.win_length,\n",
        "        #     center=False)\n",
        "        # spec = torch.squeeze(spec, 0)\n",
        "        # torch.save(spec, spec_filename)\n",
        "        return spec, audio_norm\n",
        "\n",
        "    def get_text(self, text):\n",
        "        if self.cleaned_text:\n",
        "            text_norm = cleaned_text_to_sequence(text)\n",
        "        else:\n",
        "            text_norm = text_to_sequence(text, self.text_cleaners)\n",
        "        if self.add_blank:\n",
        "            text_norm = commons.intersperse(text_norm, 0)\n",
        "        text_norm = torch.LongTensor(text_norm)\n",
        "        return text_norm\n",
        "\n",
        "    def get_sid(self, sid):\n",
        "        sid = torch.LongTensor([int(sid)])\n",
        "        return sid\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiopaths_sid_text)\n",
        "\n",
        "\n",
        "class TextAudioSpeakerCollate():\n",
        "    \"\"\" Zero-pads model inputs and targets\n",
        "    \"\"\"\n",
        "    def __init__(self, return_ids=False, no_text = False):\n",
        "        self.return_ids = return_ids\n",
        "        self.no_text = no_text\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Collate's training batch from normalized text, audio and speaker identities\n",
        "        PARAMS\n",
        "        ------\n",
        "        batch: [text_normalized, spec_normalized, wav_normalized, sid]\n",
        "        \"\"\"\n",
        "        # Right zero-pad all one-hot text sequences to max input length\n",
        "        _, ids_sorted_decreasing = torch.sort(\n",
        "            torch.LongTensor([x[1].size(1) for x in batch]),\n",
        "            dim=0, descending=True)\n",
        "\n",
        "        max_text_len = max([len(x[0]) for x in batch])\n",
        "        max_spec_len = max([x[1].size(1) for x in batch])\n",
        "        max_wav_len = max([x[2].size(1) for x in batch])\n",
        "\n",
        "        text_lengths = torch.LongTensor(len(batch))\n",
        "        spec_lengths = torch.LongTensor(len(batch))\n",
        "        wav_lengths = torch.LongTensor(len(batch))\n",
        "        sid = torch.LongTensor(len(batch))\n",
        "\n",
        "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
        "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
        "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
        "        text_padded.zero_()\n",
        "        spec_padded.zero_()\n",
        "        wav_padded.zero_()\n",
        "        for i in range(len(ids_sorted_decreasing)):\n",
        "            row = batch[ids_sorted_decreasing[i]]\n",
        "\n",
        "            text = row[0]\n",
        "            text_padded[i, :text.size(0)] = text\n",
        "            text_lengths[i] = text.size(0)\n",
        "\n",
        "            spec = row[1]\n",
        "            spec_padded[i, :, :spec.size(1)] = spec\n",
        "            spec_lengths[i] = spec.size(1)\n",
        "\n",
        "            wav = row[2]\n",
        "            wav_padded[i, :, :wav.size(1)] = wav\n",
        "            wav_lengths[i] = wav.size(1)\n",
        "\n",
        "            sid[i] = row[3]\n",
        "\n",
        "        if self.return_ids:\n",
        "            return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid, ids_sorted_decreasing\n",
        "        return text_padded, text_lengths, spec_padded, spec_lengths, wav_padded, wav_lengths, sid\n",
        "\n",
        "\n",
        "class DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n",
        "    \"\"\"\n",
        "    Maintain similar input lengths in a batch.\n",
        "    Length groups are specified by boundaries.\n",
        "    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n",
        "  \n",
        "    It removes samples which are not included in the boundaries.\n",
        "    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, boundaries, num_replicas=None, rank=None, shuffle=True):\n",
        "        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n",
        "        self.lengths = dataset.lengths\n",
        "        self.batch_size = batch_size\n",
        "        self.boundaries = boundaries\n",
        "  \n",
        "        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n",
        "        self.total_size = sum(self.num_samples_per_bucket)\n",
        "        self.num_samples = self.total_size // self.num_replicas\n",
        "  \n",
        "    def _create_buckets(self):\n",
        "        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n",
        "        for i in range(len(self.lengths)):\n",
        "            length = self.lengths[i]\n",
        "            idx_bucket = self._bisect(length)\n",
        "            if idx_bucket != -1:\n",
        "                buckets[idx_bucket].append(i)\n",
        "\n",
        "        for i in range(len(buckets) - 1, 0, -1):\n",
        "            if len(buckets[i]) == 0:\n",
        "                buckets.pop(i)\n",
        "                self.boundaries.pop(i+1)\n",
        "  \n",
        "        num_samples_per_bucket = []\n",
        "        for i in range(len(buckets)):\n",
        "            len_bucket = len(buckets[i])\n",
        "            total_batch_size = self.num_replicas * self.batch_size\n",
        "            rem = (total_batch_size - (len_bucket % total_batch_size)) % total_batch_size\n",
        "            num_samples_per_bucket.append(len_bucket + rem)\n",
        "        return buckets, num_samples_per_bucket\n",
        "  \n",
        "    def __iter__(self):\n",
        "        # deterministically shuffle based on epoch\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.epoch)\n",
        "  \n",
        "        indices = []\n",
        "        if self.shuffle:\n",
        "            for bucket in self.buckets:\n",
        "                indices.append(torch.randperm(len(bucket), generator=g).tolist())\n",
        "        else:\n",
        "            for bucket in self.buckets:\n",
        "                indices.append(list(range(len(bucket))))\n",
        "  \n",
        "        batches = []\n",
        "        for i in range(len(self.buckets)):\n",
        "            next_bucket = (i+1) % len(self.buckets)\n",
        "            bucket = self.buckets[i]\n",
        "            len_bucket = len(bucket)\n",
        "            ids_bucket = indices[i]\n",
        "            num_samples_bucket = self.num_samples_per_bucket[i]\n",
        "\n",
        "            if len_bucket == 0:\n",
        "              print(\"[Warn] Exception: length of buckets {} is 0. ID:{} Skip.\".format(i,i))\n",
        "              continue\n",
        "\n",
        "            # add extra samples to make it evenly divisible\n",
        "            rem = num_samples_bucket - len_bucket\n",
        "            ids_bucket = ids_bucket + ids_bucket * (rem // len_bucket) + ids_bucket[:(rem % len_bucket)]\n",
        "    \n",
        "            # subsample\n",
        "            ids_bucket = ids_bucket[self.rank::self.num_replicas]\n",
        "    \n",
        "            # batching\n",
        "            for j in range(len(ids_bucket) // self.batch_size):\n",
        "                batch = [bucket[idx] for idx in ids_bucket[j*self.batch_size:(j+1)*self.batch_size]]\n",
        "                batches.append(batch)\n",
        "  \n",
        "        if self.shuffle:\n",
        "            batch_ids = torch.randperm(len(batches), generator=g).tolist()\n",
        "            batches = [batches[i] for i in batch_ids]\n",
        "        self.batches = batches\n",
        "  \n",
        "        assert len(self.batches) * self.batch_size == self.num_samples\n",
        "        return iter(self.batches)\n",
        "    \n",
        "    def _bisect(self, x, lo=0, hi=None):\n",
        "      if hi is None:\n",
        "          hi = len(self.boundaries) - 1\n",
        "  \n",
        "      if hi > lo:\n",
        "          mid = (hi + lo) // 2\n",
        "          if self.boundaries[mid] < x and x <= self.boundaries[mid+1]:\n",
        "              return mid\n",
        "          elif x <= self.boundaries[mid]:\n",
        "              return self._bisect(x, lo, mid)\n",
        "          else:\n",
        "              return self._bisect(x, mid + 1, hi)\n",
        "      else:\n",
        "          return -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples // self.batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3571,
          "status": "ok",
          "timestamp": 1653907253214,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "GQSZfQwkt7_7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.util as librosa_util\n",
        "from librosa.util import normalize, pad_center, tiny\n",
        "from scipy.signal import get_window\n",
        "from scipy.io.wavfile import read\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "mel_basis = {}\n",
        "hann_window = {}\n",
        "\n",
        "\n",
        "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global hann_window\n",
        "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
        "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
        "    if wnsize_dtype_device not in hann_window:\n",
        "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
        "    return spec\n",
        "\n",
        "\n",
        "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n",
        "    global mel_basis\n",
        "    dtype_device = str(spec.dtype) + '_' + str(spec.device)\n",
        "    fmax_dtype_device = str(fmax) + '_' + dtype_device\n",
        "    if fmax_dtype_device not in mel_basis:\n",
        "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
        "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n",
        "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "    return spec\n",
        "\n",
        "\n",
        "def mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
        "    fmax_dtype_device = str(fmax) + '_' + dtype_device\n",
        "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
        "    if fmax_dtype_device not in mel_basis:\n",
        "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
        "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n",
        "    if wnsize_dtype_device not in hann_window:\n",
        "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
        "\n",
        "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_iTlRI7ShH"
      },
      "source": [
        "中身を見るためのコード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 451,
          "status": "ok",
          "timestamp": 1653907253663,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "hwb7MP-0doqF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torchaudio\n",
        "import skimage\n",
        "from skimage import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 306,
          "status": "ok",
          "timestamp": 1653907254930,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "84Kz0e6fDQB5"
      },
      "outputs": [],
      "source": [
        "SOURCE_WAVFILE = \"dataset/textful/00_myvoice/wav/emotion002.wav\"\n",
        "NET_PATH = \"./logs/20220530_da/G_10000.pth\"\n",
        "CONFIG_PATH = \"./configs/train_config_zundamon.json\"\n",
        "hps = utils.get_hparams_from_file(CONFIG_PATH)\n",
        "SOURCE_SPEAKER_ID = 107\n",
        "TARGET_ID = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 837,
          "status": "ok",
          "timestamp": 1653968638384,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "_EVMq7G9_tHM",
        "outputId": "f66c0851-3995-4c6d-9003-05a5906f6fb4"
      },
      "outputs": [],
      "source": [
        "!ls ./logs/20220530_da/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "executionInfo": {
          "elapsed": 21734,
          "status": "ok",
          "timestamp": 1653907742451,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "YZhItBqy5xZN",
        "outputId": "84b8c2e2-4ca4-48f3-e63a-655d464d7961"
      },
      "outputs": [],
      "source": [
        "# 修正してここで試す\n",
        "net_g = SynthesizerTrn(\n",
        "    len(symbols),\n",
        "    hps.data.filter_length // 2 + 1,\n",
        "    hps.train.segment_size // hps.data.hop_length,\n",
        "    n_speakers=hps.data.n_speakers,\n",
        "    **hps.model)\n",
        "_ = net_g.eval()\n",
        "\n",
        "_ = utils.load_checkpoint(NET_PATH, net_g, None)\n",
        "\n",
        "with torch.no_grad():\n",
        "    dataset = TextAudioSpeakerLoader(hps.data.validation_files_notext, hps.data)\n",
        "    data = dataset.get_audio_text_speaker_pair([SOURCE_WAVFILE, SOURCE_SPEAKER_ID, \"a\"])\n",
        "    data = TextAudioSpeakerCollate()([data])\n",
        "    x, x_lengths, spec, spec_lengths, y, y_lengths, sid_src = [x for x in data]\n",
        "    sid_tgt1 = torch.LongTensor([TARGET_ID])\n",
        "    # o_hat, y_mask, (z, z_p, z_hat)\n",
        "    #audio1 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt1)[0][0,0].data.cpu().float().numpy()\n",
        "    result1 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt1)\n",
        "    audio1 = result1[0][0,0].data.cpu().float().numpy()\n",
        "    z1 = result1[2][0]\n",
        "\n",
        "ipd.display(ipd.Audio(audio1, rate=hps.data.sampling_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 461,
          "status": "ok",
          "timestamp": 1653037701126,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "XSKoiT4sp9sY",
        "outputId": "f98f6a55-f4aa-4f7d-aec2-5348041f5802"
      },
      "outputs": [],
      "source": [
        "spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 387,
          "status": "ok",
          "timestamp": 1653806425627,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "dxeODKk5ujCa"
      },
      "outputs": [],
      "source": [
        "z_p = result1[2][1]\n",
        "z_hat = result1[2][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBUG0sa1mNCb"
      },
      "source": [
        "無圧縮元データ 24000Hz 16bit 1ch  \n",
        "48K/s  \n",
        "173M/h  \n",
        "\n",
        "192x504(2.7s)x0.5(4bit)=48K/2.7s  \n",
        "17.8K/s  \n",
        "64M/h\n",
        "\n",
        "圧縮して31K  \n",
        "11.5K/s  \n",
        "41M/h  \n",
        "\n",
        "参考：音声のみ  \n",
        "Zoom: 55M/h  \n",
        "Skype: 37M/h  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 389,
          "status": "ok",
          "timestamp": 1653805013382,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "dwwrSYJo5zMa",
        "outputId": "a19879b2-b2c0-4709-b0cf-3d6b4aa6c3bb"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 408,
          "status": "ok",
          "timestamp": 1653805026081,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "qTw3KZH68k7Z",
        "outputId": "dcd87637-a7db-41a0-89ae-db7385123efa"
      },
      "outputs": [],
      "source": [
        "z1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 391,
          "status": "ok",
          "timestamp": 1652868082569,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "K2Ucw9-mjiJc",
        "outputId": "ae11c423-0a3b-41ce-a169-5120df91dff0"
      },
      "outputs": [],
      "source": [
        "z02 = z1.squeeze(0).t()\n",
        "for i in range(len(z02) // 4):\n",
        "  z02[i*4+1] = z02[i*4]\n",
        "  z02[i*4+2] = z02[i*4]\n",
        "  z02[i*4+3] = z02[i*4]\n",
        "z02.t().unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 466,
          "status": "ok",
          "timestamp": 1652867843367,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "MgIk6Y0Ylu3n",
        "outputId": "0637ebcd-bf42-4d86-9a9a-31161e8b07f3"
      },
      "outputs": [],
      "source": [
        "z1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 252,
          "status": "ok",
          "timestamp": 1652777770161,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "efrns8CXOOeN",
        "outputId": "412a32c8-32b2-4ab6-ac84-e93e52229443"
      },
      "outputs": [],
      "source": [
        "z255 = ((z1 / 50.0) * 255.0).floor()\n",
        "z4 = z255 / 255.0 * 50.0\n",
        "z4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 384,
          "status": "ok",
          "timestamp": 1652857048974,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "F9XIYSfcQ_Jn",
        "outputId": "829995ae-3a17-49ad-8e95-ab2eaf24fc54"
      },
      "outputs": [],
      "source": [
        "z5=z1+8\n",
        "z5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 495,
          "status": "ok",
          "timestamp": 1652857108085,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "t5jzCOlm9Z5W",
        "outputId": "e51e3005-6a32-4236-d28f-575e9cd1126f"
      },
      "outputs": [],
      "source": [
        "torch.min(z5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 390,
          "status": "ok",
          "timestamp": 1652857133140,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "e0v2zZOOeNYQ",
        "outputId": "4149bdea-7dd0-4152-aaa1-23a35de5eb8c"
      },
      "outputs": [],
      "source": [
        "z6 = z5.numpy().astype('uint8')[0]\n",
        "z6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1652857138051,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "hUYDsMj1tUOG",
        "outputId": "2dbf9913-6f6b-492a-8418-73401c34ad29"
      },
      "outputs": [],
      "source": [
        "z7 = z6.flatten()\n",
        "z7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1652864468065,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "nbKO4IfS97S4",
        "outputId": "fb55bda0-e955-4479-b993-4e83c9a0690b"
      },
      "outputs": [],
      "source": [
        "vals = z7\n",
        "vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 418,
          "status": "ok",
          "timestamp": 1653804771383,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "x0cBNKnl_k7k"
      },
      "outputs": [],
      "source": [
        "def encode_4bit(vals):\n",
        "  encoded = bytearray()\n",
        "  if len(vals) % 2 != 0:\n",
        "    vals = np.append(vals, 0)\n",
        "  for i in range(len(vals) // 2):\n",
        "    byte = vals[i*2] | vals[i*2+1] << 4\n",
        "    encoded.append(byte)\n",
        "    #print(f\"{vals[i*2]} {vals[i*2+1]} : {bin(byte)}\")\n",
        "  return encoded\n",
        "\n",
        "def decode_4bit(encoded):\n",
        "  decoded = []\n",
        "  for byte in encoded:\n",
        "    low = (0x0f & byte)\n",
        "    high = byte >> 4\n",
        "    #print(f\"low:{low}, high:{high}\")\n",
        "    decoded.append(low)\n",
        "    decoded.append(high)\n",
        "  return np.array(decoded, dtype=\"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 524,
          "status": "ok",
          "timestamp": 1653804774018,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "6otkfND2ZJZA"
      },
      "outputs": [],
      "source": [
        "def encode_4bit_array(vals):\n",
        "  encoded = []\n",
        "  if len(vals) % 2 != 0:\n",
        "    vals = np.append(vals, 0)\n",
        "  for i in range(len(vals) // 2):\n",
        "    byte = vals[i*2] | vals[i*2+1] << 4\n",
        "    encoded.append(byte)\n",
        "    #print(f\"{vals[i*2]} {vals[i*2+1]} : {bin(byte)}\")\n",
        "  return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JvyUcqsaLm7"
      },
      "outputs": [],
      "source": [
        "encoded_z = []\n",
        "for vals in z6:\n",
        "  encoded_z.append(encode_4bit_array(vals))\n",
        "encoded_z = np.array(encoded_z, dtype=\"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 411,
          "status": "ok",
          "timestamp": 1652865006557,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "lGrYU8qKbi9T",
        "outputId": "d20e79ad-d85f-49e9-c0cd-19b983706d74"
      },
      "outputs": [],
      "source": [
        "img = skimage.img_as_ubyte(encoded_z)\n",
        "print(img)\n",
        "io.imsave(\"imgsample8.png\", img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 525,
          "status": "ok",
          "timestamp": 1652861018184,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "KnpUYQRlGVo6",
        "outputId": "8a7bf422-c23b-4021-b67c-22febe60c122"
      },
      "outputs": [],
      "source": [
        "vals = np.array([15, 14, 13, 12, 11, 10, 9])\n",
        "encoded = encode_4bit(vals)\n",
        "print(encoded)\n",
        "decoded = decode_4bit(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E49-XO-UI6cp"
      },
      "outputs": [],
      "source": [
        "encoded = encode_4bit(z7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-ci08MLq7dV"
      },
      "outputs": [],
      "source": [
        "f = open('binsample.dat', 'wb')\n",
        "f.write(encoded)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 371,
          "status": "ok",
          "timestamp": 1652864973373,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "VO96sx1XceSd",
        "outputId": "cfc80328-b457-4796-d071-ec59811a9f75"
      },
      "outputs": [],
      "source": [
        "img = skimage.img_as_ubyte(z6)\n",
        "print(img)\n",
        "io.imsave(\"imgsample4.png\", img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcauiMNOibRJ"
      },
      "outputs": [],
      "source": [
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 636,
          "status": "ok",
          "timestamp": 1652783245692,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "DJ7ZDKkfidgL",
        "outputId": "a22aa567-3f7d-47be-c293-e3d37d8d8dee"
      },
      "outputs": [],
      "source": [
        "img_input = cv2.imread(\"img.png\", cv2.COLOR_BGR2GRAY)\n",
        "cv2.imwrite(\"high_compression.png\", img_input, [int(cv2.IMWRITE_PNG_COMPRESSION), 9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 252,
          "status": "ok",
          "timestamp": 1652780899583,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "9Yj_HR1aOU8S",
        "outputId": "770d3070-7916-47cd-f9fa-1bff4ae57b6c"
      },
      "outputs": [],
      "source": [
        "a = z4[0]\n",
        "torch.where(a>8.0, 8.0, a.double())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 235,
          "status": "ok",
          "timestamp": 1652775029685,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "HO7zEHGT-ZZN",
        "outputId": "bc0f21eb-0360-4790-d9e8-75ac4175c19b"
      },
      "outputs": [],
      "source": [
        "#z1_max = max(abs(z1.min().numpy()), abs(z1.max().numpy()))\n",
        "#z2 = z1 / z1_max # -1 < z2 < 1 に正規化\n",
        "z3 = z1.squeeze(0).numpy()\n",
        "z3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "executionInfo": {
          "elapsed": 2840,
          "status": "ok",
          "timestamp": 1653806570588,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "e-5rG6mdDBpr",
        "outputId": "b16a920e-0eaf-451b-d13e-35cb92f85ed8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(z_p.squeeze(0).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 441,
          "status": "ok",
          "timestamp": 1653806648543,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "oTbBWMv0jmla",
        "outputId": "e6467eda-2af7-4439-955b-032ce7ca7638"
      },
      "outputs": [],
      "source": [
        "z_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 387,
          "status": "ok",
          "timestamp": 1653806906520,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "bBWJQzejD6G0"
      },
      "outputs": [],
      "source": [
        "np.savetxt(\"zp_emotion002.csv\", z_p.squeeze(0).numpy(), delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAdIexFDoeym"
      },
      "source": [
        "## 4 学習したモデルを読み込む"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faOeDsFXpql9"
      },
      "source": [
        "CONFIG_PATH に学習に利用したjsonファイルを`「./configs/****.json」`のように指定し、  \n",
        "NET_PATHに学習したモデルを`「./configs/xxxx/G_*****.pth」`のように指定してください。\n",
        "\n",
        "\n",
        "CONFIG_PATH = \"./configs/train_config_zundamon.json\"  \n",
        "CONFIG_PATH = \"./configs/train_config.json\"\n",
        "\n",
        "\n",
        "\n",
        "特に設定をいじっていない場合、CONFIG_PATHはどちらかになると思います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm-3oWmarsZt"
      },
      "outputs": [],
      "source": [
        "CONFIG_PATH = \"./configs/train_config_zundamon.json\"\n",
        "NET_PATH = \"./logs/20220516_1/G_2000.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EAsizUNsGAw"
      },
      "source": [
        "指定したファイルをもとにモデルの読み込みを行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 393,
          "status": "ok",
          "timestamp": 1653804790769,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "ecUDV8_ee8OP"
      },
      "outputs": [],
      "source": [
        "hps = utils.get_hparams_from_file(CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2572,
          "status": "ok",
          "timestamp": 1653907759728,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "UYrcO66SfCqD"
      },
      "outputs": [],
      "source": [
        "net_g = SynthesizerTrn(\n",
        "    len(symbols),\n",
        "    hps.data.filter_length // 2 + 1,\n",
        "    hps.train.segment_size // hps.data.hop_length,\n",
        "    n_speakers=hps.data.n_speakers,\n",
        "    **hps.model)\n",
        "_ = net_g.eval()\n",
        "\n",
        "_ = utils.load_checkpoint(NET_PATH, net_g, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6XCTRpgsTII"
      },
      "source": [
        "## 5 学習したモデルでTTSを行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1wGpP8te8d"
      },
      "source": [
        "TEXTに**ひらがな**で文章を入力してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 252,
          "status": "ok",
          "timestamp": 1653907755091,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "kIy1xAiItn7i"
      },
      "outputs": [],
      "source": [
        "TEXT = \"おはよう、しょくん。わたしはてんのうずあいるだ。これはおんせいごうせいのてすとだ。どうだ？すごいだろう。\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQuihULMdjX5"
      },
      "source": [
        "TTS_Speaker_IDにTTSを実行する話者IDを代入してください。  \n",
        "(filelists/xxxx_Correspondence.txtに話者Idと対応した話者の対応が出力されています！)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 255,
          "status": "ok",
          "timestamp": 1653907763366,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "Sx0_TsvKd8Oc"
      },
      "outputs": [],
      "source": [
        "TTS_Speaker_ID = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fILJqQ6pt5bX"
      },
      "source": [
        "実際にTTSを行います。\n",
        "\n",
        "イントネーションがおかしくなることが多々ありますが、VC的には問題なので気にしなくて大丈夫です。\n",
        "\n",
        "初回のみpyopenjtalk周りのダウンロードが始まるので、少し時間がかかりますが仕様です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "executionInfo": {
          "elapsed": 13662,
          "status": "ok",
          "timestamp": 1653907803797,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "DXeNhEV7fI36",
        "outputId": "8db69ff0-c5ce-4820-d882-6c21934e3bf3"
      },
      "outputs": [],
      "source": [
        "text = TEXT\n",
        "stn_tst = get_text(mozi2phone(text), hps)\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_tst = stn_tst.unsqueeze(0)\n",
        "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\n",
        "    sid = torch.LongTensor([TTS_Speaker_ID])\n",
        "    audio = net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.400, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float().numpy()\n",
        "ipd.display(ipd.Audio(audio, rate=hps.data.sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ho5133vpFi"
      },
      "source": [
        "## 6 学習したモデルで非リアルタイムVCを行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRtrz7GIwyGq"
      },
      "source": [
        "非リアルタイムのVCを行います。\n",
        "\n",
        "ソース話者のIDとその話者の音声ファイルのパス、変換ターゲットの話者のIDを指定してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEqm8yA6v9xz"
      },
      "outputs": [],
      "source": [
        "SOURCE_WAVFILE = \"dataset/textful/00_myvoice/wav/emotion002.wav\"\n",
        "SOURCE_SPEAKER_ID = 107\n",
        "TARGET_ID = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHy-FQAYxLOR"
      },
      "source": [
        "実際にVCを行います。\n",
        "\n",
        "ここでの性能が悪い場合、学習不足か他に問題があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "executionInfo": {
          "elapsed": 5,
          "status": "error",
          "timestamp": 1653030115127,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "2vkotLtNY_s4",
        "outputId": "00eb2fee-05f1-44a5-d75b-38da71f41163"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    dataset = TextAudioSpeakerLoader(hps.data.validation_files_notext, hps.data)\n",
        "    data = dataset.get_audio_text_speaker_pair([SOURCE_WAVFILE, SOURCE_SPEAKER_ID, \"a\"])\n",
        "    data = TextAudioSpeakerCollate()([data])\n",
        "    x, x_lengths, spec, spec_lengths, y, y_lengths, sid_src = [x for x in data]\n",
        "    sid_tgt1 = torch.LongTensor([TARGET_ID])\n",
        "    audio1 = net_g.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt1)[0][0,0].data.cpu().float().numpy()\n",
        "print(\"Original SID: %d\" % sid_src.item())\n",
        "ipd.display(ipd.Audio(y[0].cpu().numpy(), rate=hps.data.sampling_rate))\n",
        "print(\"Converted SID: %d\" % sid_tgt1.item())\n",
        "ipd.display(ipd.Audio(audio1, rate=hps.data.sampling_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Augmentationのテスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "executionInfo": {
          "elapsed": 851,
          "status": "ok",
          "timestamp": 1652764355131,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "gxuw8A3KFLDz",
        "outputId": "909d2a59-b202-4407-8dba-82fbfc973533"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "sampling_rate = 24000\n",
        "speech, rate = librosa.load(SOURCE_WAVFILE, sr=sampling_rate)\n",
        "librosa.display.waveplot(speech, sr=rate)\n",
        "plt.show()\n",
        "ipd.display(ipd.Audio(speech, rate=rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 253,
          "status": "ok",
          "timestamp": 1652770447285,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "f68MxaCQy2K-",
        "outputId": "1595054b-d6d1-4e61-e6f1-29971984a93f"
      },
      "outputs": [],
      "source": [
        "speech.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "executionInfo": {
          "elapsed": 293,
          "status": "ok",
          "timestamp": 1652770557612,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "nvDv2ZSOyxJK",
        "outputId": "a2548b03-1786-4fe9-bb91-8017ec41cdb5"
      },
      "outputs": [],
      "source": [
        "librosa.display.waveplot(speech[5950:6000], sr=rate)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "executionInfo": {
          "elapsed": 1052,
          "status": "ok",
          "timestamp": 1652764362428,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "kEIGLnHy_j0l",
        "outputId": "fa4eef2c-1dbe-44d4-fe30-cbdbef2cd239"
      },
      "outputs": [],
      "source": [
        "D = librosa.stft(speech)\n",
        "S, phase = librosa.magphase(D)\n",
        "Sdb = librosa.amplitude_to_db(S)\n",
        "librosa.display.specshow(Sdb, sr=rate, x_axis=\"time\", y_axis=\"log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 251,
          "status": "ok",
          "timestamp": 1652764924677,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "zYWq2Zh6dzAa",
        "outputId": "c6a2af8b-d038-4255-80e9-6208f39f221e"
      },
      "outputs": [],
      "source": [
        "Sdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enqMUq9aFPeQ"
      },
      "outputs": [],
      "source": [
        "# torchaudio でspectrogram\n",
        "# https://github.com/zassou65535/VITS/blob/main/vits_voice_converter.py\n",
        "\n",
        "#スペクトログラムの計算時に何サンプル単位でSTFTを行うか\n",
        "filter_length = 1024\n",
        "#スペクトログラムの計算時に適用する窓の大きさ\n",
        "win_length = 1024\n",
        "#ホップ数　何サンプルずらしながらSTFTを行うか\n",
        "hop_length = 256\n",
        "\n",
        "wav, _ = torchaudio.load(SOURCE_WAVFILE)\n",
        "#wav.shape\n",
        "pad_size = int((filter_length - hop_length)/2)\n",
        "wav_padded = torch.nn.functional.pad(wav, (pad_size, pad_size), mode=\"reflect\")\n",
        "spec = torchaudio.functional.spectrogram(\n",
        "    waveform=wav_padded, pad=0, window=torch.hann_window(win_length), n_fft=filter_length, hop_length=hop_length, win_length=win_length, power=2, normalized=False, center=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "executionInfo": {
          "elapsed": 680,
          "status": "ok",
          "timestamp": 1652765845570,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "2E7pr4qxga04",
        "outputId": "230b6ac1-c0b8-4102-d135-0b8203598754"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(wav.t().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 248,
          "status": "ok",
          "timestamp": 1652769502780,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "S4AXP6cNdtFX",
        "outputId": "5cf0f88d-1eeb-45c3-c262-8a4dcb8a7aad"
      },
      "outputs": [],
      "source": [
        "spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 237,
          "status": "ok",
          "timestamp": 1652765351173,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "JEXJuN74eogt",
        "outputId": "5149e266-9110-4e4f-bd79-a6614b8ab560"
      },
      "outputs": [],
      "source": [
        "spec.squeeze(0).numpy()*10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "executionInfo": {
          "elapsed": 321,
          "status": "ok",
          "timestamp": 1652765527280,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "4WlC-Bo8dar9",
        "outputId": "51293bf4-8a08-4974-f7c2-900da6b2bbdb"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(spec.squeeze(0).numpy()*-100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "executionInfo": {
          "elapsed": 405,
          "status": "error",
          "timestamp": 1652771208756,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "OouareonB3pm",
        "outputId": "d3104e47-9cbe-4664-9748-d82497bb35b2"
      },
      "outputs": [],
      "source": [
        "y = wav.t()\n",
        "\n",
        "#torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
        "#                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
        "#    \"sampling_rate\": 24000,\n",
        "#    \"filter_length\": 512,\n",
        "#    \"hop_length\": 128,\n",
        "#    \"win_length\": 512,\n",
        "#    \"n_mel_channels\": 80,\n",
        "#    \"mel_fmin\": 0.0,\n",
        "#    \"mel_fmax\": null,\n",
        "n_fft =         512 # hps.data.filter_length\n",
        "num_mels =      80  # hps.data.n_mel_channels\n",
        "sampling_rate = 24000 # hps.data.sampling_rate\n",
        "hop_size =      128 # hps.data.hop_length\n",
        "win_size =      512 # hps.data.win_length\n",
        "fmin =          0.0 # hps.data.mel_fmin\n",
        "fmax =          None # hps.data.mel_fmax\n",
        "center =        False\n",
        "\n",
        "hann_window = {}\n",
        "dtype_device = str(y.dtype) + '_' + str(y.device)\n",
        "fmax_dtype_device = str(fmax) + '_' + dtype_device\n",
        "wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
        "hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
        "torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "executionInfo": {
          "elapsed": 347,
          "status": "error",
          "timestamp": 1652689367168,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "2r9fKDkh1MaM",
        "outputId": "6a70c41b-1dc7-4da5-a8eb-57c3469f653c"
      },
      "outputs": [],
      "source": [
        "# def mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "\n",
        "# n_fft :         hps.data.filter_length, \n",
        "# num_mels :      hps.data.n_mel_channels, \n",
        "# sampling_rate : hps.data.sampling_rate, \n",
        "# hop_size :      hps.data.hop_length, \n",
        "# win_size :      hps.data.win_length, \n",
        "# fmin :          hps.data.mel_fmin, \n",
        "# fmax :          hps.data.mel_fmax\n",
        "\n",
        "from mel_processing import mel_spectrogram_torch, spec_to_mel_torch\n",
        "\n",
        "speech_mel = mel_spectrogram_torch(\n",
        "    speech.squeeze(1), \n",
        "    hps.data.filter_length, \n",
        "    hps.data.n_mel_channels, \n",
        "    hps.data.sampling_rate, \n",
        "    hps.data.hop_length, \n",
        "    hps.data.win_length, \n",
        "    hps.data.mel_fmin, \n",
        "    hps.data.mel_fmax\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edG0LSwA23QM"
      },
      "outputs": [],
      "source": [
        "def mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global mel_basis, hann_window\n",
        "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
        "    fmax_dtype_device = str(fmax) + '_' + dtype_device\n",
        "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
        "    if fmax_dtype_device not in mel_basis:\n",
        "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
        "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n",
        "    if wnsize_dtype_device not in hann_window:\n",
        "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    print(f\"(mel_spectrogram_torch) n_fft: {n_fft}\")\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
        "\n",
        "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MMVC_Interface_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
