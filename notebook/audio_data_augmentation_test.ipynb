{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1653888117313,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "ApL706JcveCy"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Nh484xveC0"
      },
      "source": [
        "\n",
        "Audio Data Augmentation\n",
        "=================\n",
        "\n",
        "``torchaudio`` provides a variety of ways to augment audio data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3674,
          "status": "ok",
          "timestamp": 1653888122889,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "eq_jyi94veC1",
        "outputId": "09a0d66f-e825-43da-9035-52431c023d46"
      },
      "outputs": [],
      "source": [
        "# When running this tutorial in Google Colab, install the required packages\n",
        "# with the following.\n",
        "# !pip install torchaudio\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import audiomentations\n",
        "import librosa\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP0P240sveC2"
      },
      "source": [
        "Preparing data and utility functions (skip this section)\n",
        "--------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 1320,
          "status": "ok",
          "timestamp": 1653888198752,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "BGXJkVRBveC2",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "    axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "      axes[c].set_ylim(ylim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def print_stats(waveform, sample_rate=None, src=None):\n",
        "  if src:\n",
        "    print(\"-\" * 10)\n",
        "    print(\"Source:\", src)\n",
        "    print(\"-\" * 10)\n",
        "  if sample_rate:\n",
        "    print(\"Sample Rate:\", sample_rate)\n",
        "  print(\"Shape:\", tuple(waveform.shape))\n",
        "  print(\"Dtype:\", waveform.dtype)\n",
        "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
        "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
        "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
        "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
        "  print()\n",
        "  print(waveform)\n",
        "  print()\n",
        "\n",
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  if num_channels == 1:\n",
        "    display(Audio(waveform[0], rate=sample_rate))\n",
        "  elif num_channels == 2:\n",
        "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "  else:\n",
        "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37n51NmkveC3"
      },
      "source": [
        "Applying effects and filtering\n",
        "------------------------------\n",
        "\n",
        "``torchaudio.sox_effects`` allows for directly applying filters similar to\n",
        "those available in ``sox`` to Tensor objects and file object audio sources.\n",
        "\n",
        "There are two functions for this:\n",
        "\n",
        "-  ``torchaudio.sox_effects.apply_effects_tensor`` for applying effects\n",
        "   to Tensor.\n",
        "-  ``torchaudio.sox_effects.apply_effects_file`` for applying effects to\n",
        "   other audio sources.\n",
        "\n",
        "Both functions accept effect definitions in the form\n",
        "``List[List[str]]``.\n",
        "This is mostly consistent with how ``sox`` command works, but one caveat is\n",
        "that ``sox`` adds some effects automatically, whereas ``torchaudio``’s\n",
        "implementation does not.\n",
        "\n",
        "For the list of available effects, please refer to `the sox\n",
        "documentation <http://sox.sourceforge.net/sox.html>`__.\n",
        "\n",
        "**Tip** If you need to load and resample your audio data on the fly,\n",
        "then you can use ``torchaudio.sox_effects.apply_effects_file`` with\n",
        "effect ``\"rate\"``.\n",
        "\n",
        "**Note** ``apply_effects_file`` accepts a file-like object or path-like\n",
        "object. Similar to ``torchaudio.load``, when the audio format cannot be\n",
        "inferred from either the file extension or header, you can provide\n",
        "argument ``format`` to specify the format of the audio source.\n",
        "\n",
        "**Note** This process is not differentiable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 927,
          "status": "ok",
          "timestamp": 1653889619628,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "XMjGiAibgMnG",
        "outputId": "8d914af5-9849-4c0e-d930-7b5919f73f57"
      },
      "outputs": [],
      "source": [
        "%cd ../\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 2806,
          "status": "ok",
          "timestamp": 1653893769948,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "Cn-A1_gBaz31",
        "outputId": "5dd8009f-9fef-4070-b3e7-9262dc62c56a"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 24000\n",
        "\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read\n",
        "import random\n",
        "\n",
        "def load_wav_to_torch(full_path):\n",
        "  sampling_rate, data = read(full_path)\n",
        "  return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
        "\n",
        "hann_window = {}\n",
        "\n",
        "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
        "    if torch.min(y) < -1.:\n",
        "        print('min value is ', torch.min(y))\n",
        "    if torch.max(y) > 1.:\n",
        "        print('max value is ', torch.max(y))\n",
        "\n",
        "    global hann_window\n",
        "    dtype_device = str(y.dtype) + '_' + str(y.device)\n",
        "    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n",
        "    if wnsize_dtype_device not in hann_window:\n",
        "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=True)\n",
        "    spec = torch.view_as_real(spec)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
        "    return spec\n",
        "\n",
        "augmentation_effects = [\n",
        "    ['delay', '128s'],\n",
        "    ['gain', '-6'],\n",
        "    [\"tempo\", \"0.8\"],\n",
        "    [\"pitch\", \"400\"],\n",
        "    [\"rate\", f\"{sampling_rate}\"]\n",
        "]\n",
        "\n",
        "def add_augmentation(audio, sampling_rate):\n",
        "    global augmentation_effects\n",
        "    audio_augmented, _ = torchaudio.sox_effects.apply_effects_tensor(audio, sampling_rate, augmentation_effects)\n",
        "    return audio_augmented\n",
        "\n",
        "def add_noise(audio, sampling_rate):\n",
        "    # AddGaussianNoise\n",
        "    noised_audio = add_gaussian_noise(audio)\n",
        "    return noised_audio\n",
        "\n",
        "def add_gaussian_noise(audio):\n",
        "    #amplitude = random.uniform(0.001, 0.004)\n",
        "    amplitude = random.uniform(0.0, 0.0)\n",
        "    noise = torch.randn(audio.size())\n",
        "    noised_audio = audio + amplitude * noise\n",
        "    return noised_audio\n",
        "\n",
        "def add_spectrogram_noise(spec):\n",
        "    # FrequencyMask\n",
        "    masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=80)\n",
        "    masked = masking(spec)\n",
        "    return masked\n",
        "\n",
        "max_wav_value = 32768\n",
        "def get_normalized_audio(audio, max_wav_value):\n",
        "    audio_norm = audio / max_wav_value\n",
        "    audio_norm = audio_norm.unsqueeze(0)\n",
        "    return audio_norm\n",
        "\n",
        "def plot_specgram2(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "#effects = [\n",
        "#  [\"lowpass\", \"-1\", \"300\"], # ローパスフィルター(単極)の適用\n",
        "#  [\"speed\", \"0.8\"],  # 速度を下げる\n",
        "#  [\"rate\", f\"{sampling_rate}\"],\n",
        "#  [\"reverb\", \"-w\"],  # 反響のエフェクトを加えるとドラマチックな雰囲気になる\n",
        "#]\n",
        "\n",
        "SOURCE_WAVFILE = \"dataset/textful/00_myvoice/wav/emotion002.wav\"\n",
        "\n",
        "speech_torch, sr = load_wav_to_torch(SOURCE_WAVFILE)\n",
        "\n",
        "#    audio_augmented = self.add_augumentation(audio, sampling_rate)\n",
        "#    audio_noised = self.add_noise(audio_augmented, sampling_rate)\n",
        "#    audio_augmented = np.clip(audio_augmented, -self.max_wav_value, self.max_wav_value)\n",
        "#    audio_noised = np.clip(audio_noised, -self.max_wav_value, self.max_wav_value)\n",
        "#    audio_augmented_norm = self.get_normalized_audio(audio_augmented, self.max_wav_value)\n",
        "#    audio_noised_norm = self.get_normalized_audio(audio_noised, self.max_wav_value)\n",
        "#    audio = audio_augmented_norm\n",
        "#    spec = spectrogram_torch(audio_noised_norm, self.filter_length,\n",
        "#        self.sampling_rate, self.hop_length, self.win_length,\n",
        "#        center=False)\n",
        "#    spec_noised = self.add_spectrogram_noise(spec)\n",
        "#    spec = torch.squeeze(spec_noised, 0)\n",
        "\n",
        "#    else:\n",
        "audio_norm = get_normalized_audio(speech_torch, max_wav_value) # plotとかすんのには余計に１次元でくるんでやる必要がある\n",
        "audio_augmented = add_augmentation(audio_norm, sampling_rate)\n",
        "audio_noised = add_noise(audio_augmented, sampling_rate)\n",
        "audio_augmented = torch.clamp(audio_augmented, -1, 1) \n",
        "audio_noised = torch.clamp(audio_noised, -1, 1)\n",
        "\n",
        "plot_waveform(audio_norm, sr, title=\"Original\", ylim=None)\n",
        "plot_specgram(audio_norm, sr, title=\"Original\")\n",
        "play_audio(audio_norm, sr)\n",
        "\n",
        "plot_waveform(audio_noised, sr, title=\"Augmented torchaudio\", ylim=None)\n",
        "plot_specgram(audio_noised, sr, title=\"Augmented torchaudio\")\n",
        "play_audio(audio_noised, sr)\n",
        "\n",
        "# スペクトログラム\n",
        "spec = spectrogram_torch(audio_noised, 512, sampling_rate, 128, 512, center=False)\n",
        "spec = torch.squeeze(spec, 0)\n",
        "\n",
        "print(\"Shape of spectrogram: {}\".format(spec.size()))\n",
        "plt.figure()\n",
        "p = plt.imshow(spec.log2().numpy(), origin='lower')\n",
        "\n",
        "masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=80)\n",
        "spec = masking(spec)\n",
        "\n",
        "print(\"masked hape of spectrogram: {}\".format(spec.size()))\n",
        "plt.figure()\n",
        "p = plt.imshow(spec.log2().numpy(), origin='lower')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = random.randint(0, 100)\n",
        "print(f\"{test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SOURCE_WAVFILE = \"dataset/textful/00_myvoice/wav/emotion002.wav\"\n",
        "speech_torch, sr = load_wav_to_torch(SOURCE_WAVFILE)\n",
        "audio = get_normalized_audio(speech_torch, max_wav_value) # plotとかすんのには余計に１次元でくるんでやる必要がある\n",
        "\n",
        "# スペクトログラム\n",
        "spec = spectrogram_torch(audio, 512, sampling_rate, 128, 512, center=False)\n",
        "spec = torch.squeeze(spec, 0)\n",
        "\n",
        "print(\"Shape of spectrogram: {}\".format(spec.size()))\n",
        "plt.figure()\n",
        "p = plt.imshow(spec.log2().numpy(), origin='lower')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_frames = audio.unfold(1, 8192, 8192)\n",
        "audio_width = audio.size()[1]\n",
        "padded_audio = torch.zeros(1, audio_width + 256) # 前の win_length / 2 ぶん増やす\n",
        "padded_audio[:,256:] = audio\n",
        "ol_audio_frames = padded_audio.unfold(1, 8192+512, 8192) # 前後の win_length / 2 ぶん切り出す量を増やす\n",
        "spec_frames = spec.unfold(1, 64, 64).transpose(0, 1)\n",
        "\n",
        "# spec -> split (origin)\n",
        "print(\"spec->split\")\n",
        "plt.figure()\n",
        "for i in range(0,7):\n",
        "    spec_frame = spec_frames[i]\n",
        "#    print(spec_frame)\n",
        "#    plt.subplot(1, 4, i+1)\n",
        "#    plt.imshow(spec_frame.log2().numpy(), origin='lower')\n",
        "\n",
        "cated_spec = None\n",
        "for i in range(0,7):\n",
        "    if cated_spec == None:\n",
        "        cated_spec = spec_frames[i]\n",
        "    else:\n",
        "        cated_spec = torch.cat((cated_spec, spec_frames[i]), 1)\n",
        "cated_spec = cated_spec.squeeze(0)\n",
        "print(cated_spec.size())\n",
        "plt.imshow(cated_spec.log2().numpy(), origin='lower')\n",
        "\n",
        "# split -> spec (realtime conv)\n",
        "print(\"split->spec\")\n",
        "specs = {}\n",
        "plt.figure()\n",
        "for i in range(0,7):\n",
        "    specs[i] = spectrogram_torch(audio_frames[0][i].unsqueeze(0), 512, sampling_rate, 128, 512, center=False)\n",
        "    spec_frame = torch.squeeze(specs[i], 0)\n",
        "#    print(spec_frame)\n",
        "#    plt.subplot(1, 4, i+1)\n",
        "#    plt.imshow(spec_frame.log2().numpy(), origin='lower')\n",
        "\n",
        "cated_spec = None\n",
        "for i in range(0,7):\n",
        "    if cated_spec == None:\n",
        "        cated_spec = specs[i]\n",
        "    else:\n",
        "        cated_spec = torch.cat((cated_spec, specs[i]), 2)\n",
        "cated_spec = cated_spec.squeeze(0)\n",
        "print(cated_spec.size())\n",
        "plt.imshow(cated_spec.log2().numpy(), origin='lower')\n",
        "\n",
        "# diff\n",
        "#print(\"diff\")\n",
        "#plt.figure()\n",
        "for i in range(0,7):\n",
        "    diff = (spec_frames[i] - torch.squeeze(specs[i], 0))\n",
        "#    print(diff)\n",
        "#    ax = plt.subplot(1, 4, i+1)\n",
        "#    plt.imshow(diff.log2().numpy(), origin='lower')\n",
        "\n",
        "# overlap split -> spec (realtime conv)\n",
        "print(\"overlap split->spec\")\n",
        "specs = {}\n",
        "plt.figure()\n",
        "for i in range(0,7):\n",
        "    specs[i] = spectrogram_torch(ol_audio_frames[0][i].unsqueeze(0), 512, sampling_rate, 128, 512, center=False)\n",
        "    specs[i] = specs[i][:, :, 2:66] # 頭と終わりの2個づつ捨てる\n",
        "    spec_frame = torch.squeeze(specs[i], 0)\n",
        "#    print(spec_frame)\n",
        "    print(spec_frame.size())\n",
        "#    plt.subplot(1, 4, i+1)\n",
        "#    plt.imshow(spec_frame.log2().numpy(), origin='lower')\n",
        "\n",
        "cated_spec = None\n",
        "for i in range(0,7):\n",
        "    if cated_spec == None:\n",
        "        cated_spec = specs[i]\n",
        "    else:\n",
        "        cated_spec = torch.cat((cated_spec, specs[i]), 2)\n",
        "cated_spec = cated_spec.squeeze(0)\n",
        "print(cated_spec.size())\n",
        "plt.imshow(cated_spec.log2().numpy(), origin='lower')\n",
        "\n",
        "# ol diff\n",
        "print(\"ol diff\")\n",
        "#plt.figure()\n",
        "for i in range(0,7):\n",
        "    diff = (spec_frames[i] - torch.squeeze(specs[i], 0))\n",
        "    print(diff)\n",
        "#    ax = plt.subplot(1, 4, i+1)\n",
        "#    plt.imshow(diff.log2().numpy(), origin='lower')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x1 = torch.arange(0., 10).unsqueeze(0)\n",
        "x2 = torch.arange(10., 20).unsqueeze(0)\n",
        "x3 = torch.arange(20., 30).unsqueeze(0)\n",
        "x4 = torch.arange(30., 40).unsqueeze(0)\n",
        "#print(x1, x2)\n",
        "xcat = torch.cat((x1, x2, x3, x4), 0)\n",
        "print(xcat)\n",
        "xtest = xcat[:, 1:8]\n",
        "print(xtest)\n",
        "\n",
        "frame_width = 2\n",
        "spec_height = xcat.size()[0]\n",
        "spec_width = xcat.size()[1]\n",
        "print(spec_width)\n",
        "padded_spec_width = spec_width + frame_width - spec_width % frame_width\n",
        "print(padded_spec_width)\n",
        "frame_size = padded_spec_width // frame_width\n",
        "padded_spec = torch.zeros(spec_height, padded_spec_width)\n",
        "print(padded_spec.size())\n",
        "padded_spec[:, :spec_width] = xcat\n",
        "print(padded_spec)\n",
        "\n",
        "spec_frames = padded_spec.reshape(frame_size, spec_height, frame_width)\n",
        "print(spec_frames)\n",
        "\n",
        "xcat.unfold(1, 3, 3)\n",
        "#xtuple = xcat.split(2, dim=0)\n",
        "#for x in xtuple:\n",
        "#    x = x.unsqueeze(0)\n",
        "#    print(x)\n",
        "#xsplits = torch.cat(xtuple, -1)\n",
        "#print(xsplits)\n",
        "#xtuple\n",
        "#torch.cat((xtuple[0].unsqueeze(0), xtuple[1].unsqueeze(0)), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "bytes1 = b'abcdefg'\n",
        "bytes2 = b'tuvwxyz'\n",
        "bytes3 = bytes1 + bytes2\n",
        "bytes4 = bytes3[4:8]\n",
        "bytes4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bytescat = bytes1+bytes2\n",
        "bytestail = bytescat[-3:]\n",
        "bytestail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "リアルタイム音声変換でファイルから入力する場合のモック作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockStream:\n",
        "    def __init__(self, input_filename, output_filename):\n",
        "        self.fr = open(input_filename, 'rb')\n",
        "        self.fw = open(output_filename, 'wb')\n",
        "        \n",
        "    def read(self, length, exception_on_overflow=False):\n",
        "        wav = self.fr.read(length)\n",
        "        if len(wav) <= 0:\n",
        "            raise ValueError(\"End of data.\")\n",
        "        return wav\n",
        "\n",
        "    def write(self, wav):\n",
        "        self.fw.write(wav)\n",
        "    \n",
        "test_stream = MockStream(\"testinput.wav\", \"testoutput.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    for i in range(100):\n",
        "        wav = test_stream.read(8192)\n",
        "        test_stream.write(wav)\n",
        "\n",
        "        signal = np.frombuffer(wav, dtype='int16')\n",
        "        wav_torch = torch.FloatTensor(signal.astype(np.float32))\n",
        "        audio = get_normalized_audio(wav_torch, max_wav_value)\n",
        "        spec = spectrogram_torch(audio, 512, sampling_rate, 128, 512, center=False)\n",
        "        spec = torch.squeeze(spec, 0)\n",
        "\n",
        "        print(\"Shape of spectrogram: {}\".format(spec.size()))\n",
        "        plt.figure()\n",
        "        p = plt.imshow(spec.log2().numpy(), origin='lower')\n",
        "except Exception as e:\n",
        "    print(\"Stop Streaming: \" +  repr(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestA:\n",
        "    def __init__(self):\n",
        "        self.test1 = None\n",
        "        self.test2 = None\n",
        "        \n",
        "    def test(self):\n",
        "        if 'test1' in locals():\n",
        "            print(\"test1 in\")\n",
        "        if 'test3' in locals():\n",
        "            print(\"test3 in \")\n",
        "\n",
        "a = TestA()\n",
        "a.test1 = \"testa1\"\n",
        "a.test2 = \"testa2\"\n",
        "\n",
        "a.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if hasattr(a, \"test1\"):\n",
        "    print(\"OK\")\n",
        "else:\n",
        "    print(\"NO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1418,
          "status": "ok",
          "timestamp": 1653810302239,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "jAMtQUGnveC4",
        "outputId": "40cd4623-7cd1-4b92-bdec-1d0dd755df89"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "waveform1, sample_rate1 = get_sample(resample=16000)\n",
        "\n",
        "# Define effects\n",
        "effects = [\n",
        "  [\"lowpass\", \"-1\", \"300\"], # apply single-pole lowpass filter\n",
        "  ['gain', '+20'],  # normalises to 0dB\n",
        "  ['pitch', '-50'],  # 5 cent pitch shift\n",
        "  [\"speed\", \"1.0\"],  # reduce the speed\n",
        "                     # This only changes sample rate, so it is necessary to\n",
        "                     # add `rate` effect with original sample rate after this.\n",
        "  [\"rate\", f\"{sample_rate1}\"],\n",
        "  [\"reverb\", \"-w\"],  # Reverbration gives some dramatic feeling\n",
        "]\n",
        "\n",
        "# Apply effects\n",
        "waveform2, sample_rate2 = torchaudio.sox_effects.apply_effects_tensor(\n",
        "    waveform1, sample_rate1, effects)\n",
        "\n",
        "plot_waveform(waveform1, sample_rate1, title=\"Original\", xlim=(-.1, 3.2))\n",
        "plot_waveform(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(-.1, 3.2))\n",
        "print_stats(waveform1, sample_rate=sample_rate1, src=\"Original\")\n",
        "print_stats(waveform2, sample_rate=sample_rate2, src=\"Effects Applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Z6LY06veC4"
      },
      "source": [
        "Note that the number of frames and number of channels are different from\n",
        "those of the original after the effects are applied. Let’s listen to the\n",
        "audio. Doesn’t it sound more dramatic?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "executionInfo": {
          "elapsed": 1444,
          "status": "ok",
          "timestamp": 1653810306324,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -540
        },
        "id": "9NgcwFT5veC5",
        "outputId": "48831b50-fe8b-4fb2-a4be-819e06cfb7a9"
      },
      "outputs": [],
      "source": [
        "plot_specgram(waveform1, sample_rate1, title=\"Original\", xlim=(0, 3.04))\n",
        "play_audio(waveform1, sample_rate1)\n",
        "plot_specgram(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(0, 3.04))\n",
        "play_audio(waveform2, sample_rate2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c6hZ49eveC5"
      },
      "source": [
        "Simulating room reverberation\n",
        "----------------------------\n",
        "\n",
        "`Convolution\n",
        "reverb <https://en.wikipedia.org/wiki/Convolution_reverb>`__ is a\n",
        "technique that's used to make clean audio sound as though it has been\n",
        "produced in a different environment.\n",
        "\n",
        "Using Room Impulse Response (RIR), for instance, we can make clean speech\n",
        "sound as though it has been uttered in a conference room.\n",
        "\n",
        "For this process, we need RIR data. The following data are from the VOiCES\n",
        "dataset, but you can record your own — just turn on your microphone\n",
        "and clap your hands.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "executionInfo": {
          "elapsed": 249,
          "status": "error",
          "timestamp": 1653888170994,
          "user": {
            "displayName": "Kiyoshi SATOH",
            "userId": "17263090257850830810"
          },
          "user_tz": -540
        },
        "id": "UVNsSu0xveC5",
        "outputId": "0b332252-9f0f-4c66-ff02-7fee75b99c5e"
      },
      "outputs": [],
      "source": [
        "sample_rate = 8000\n",
        "\n",
        "rir_raw, _ = get_rir_sample(resample=sample_rate)\n",
        "\n",
        "plot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\", ylim=None)\n",
        "plot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\n",
        "play_audio(rir_raw, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB8VdMHrveC6"
      },
      "source": [
        "First, we need to clean up the RIR. We extract the main impulse, normalize\n",
        "the signal power, then flip along the time axis.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha3h_s58veC6"
      },
      "outputs": [],
      "source": [
        "rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
        "rir = rir / torch.norm(rir, p=2)\n",
        "rir = torch.flip(rir, [1])\n",
        "\n",
        "print_stats(rir)\n",
        "plot_waveform(rir, sample_rate, title=\"Room Impulse Response\", ylim=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP5tF9QGveC6"
      },
      "source": [
        "Then, we convolve the speech signal with the RIR filter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dToeRoL-veC7"
      },
      "outputs": [],
      "source": [
        "speech, _ = get_speech_sample(resample=sample_rate)\n",
        "\n",
        "speech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\n",
        "augmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
        "\n",
        "plot_waveform(speech, sample_rate, title=\"Original\", ylim=None)\n",
        "plot_waveform(augmented, sample_rate, title=\"RIR Applied\", ylim=None)\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"Original\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "plot_specgram(augmented, sample_rate, title=\"RIR Applied\")\n",
        "play_audio(augmented, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXJg5_aFveC7"
      },
      "source": [
        "Adding background noise\n",
        "-----------------------\n",
        "\n",
        "To add background noise to audio data, you can simply add a noise Tensor to\n",
        "the Tensor representing the audio data. A common method to adjust the\n",
        "intensity of noise is changing the Signal-to-Noise Ratio (SNR).\n",
        "[`wikipedia <https://en.wikipedia.org/wiki/Signal-to-noise_ratio>`__]\n",
        "\n",
        "\\begin{align}\\mathrm{SNR} = \\frac{P_\\mathrm{signal}}{P_\\mathrm{noise}}\\end{align}\n",
        "\n",
        "\\begin{align}{\\mathrm  {SNR_{{dB}}}}=10\\log _{{10}}\\left({\\mathrm  {SNR}}\\right)\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miX6jU4eveC7"
      },
      "outputs": [],
      "source": [
        "sample_rate = 8000\n",
        "speech, _ = get_speech_sample(resample=sample_rate)\n",
        "noise, _ = get_noise_sample(resample=sample_rate)\n",
        "noise = noise[:, :speech.shape[1]]\n",
        "\n",
        "plot_waveform(noise, sample_rate, title=\"Background noise\")\n",
        "plot_specgram(noise, sample_rate, title=\"Background noise\")\n",
        "play_audio(noise, sample_rate)\n",
        "\n",
        "speech_power = speech.norm(p=2)\n",
        "noise_power = noise.norm(p=2)\n",
        "\n",
        "for snr_db in [20, 10, 3]:\n",
        "  snr = math.exp(snr_db / 10)\n",
        "  scale = snr * noise_power / speech_power\n",
        "  noisy_speech = (scale * speech + noise) / 2\n",
        "\n",
        "  plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "  plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n",
        "  play_audio(noisy_speech, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOnPny73veC8"
      },
      "source": [
        "Applying codec to Tensor object\n",
        "-------------------------------\n",
        "\n",
        "``torchaudio.functional.apply_codec`` can apply codecs to a Tensor object.\n",
        "\n",
        "**Note** This process is not differentiable.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-38IfWUxveC8"
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = get_speech_sample(resample=8000)\n",
        "\n",
        "plot_specgram(waveform, sample_rate, title=\"Original\")\n",
        "play_audio(waveform, sample_rate)\n",
        "\n",
        "configs = [\n",
        "    ({\"format\": \"wav\", \"encoding\": 'ULAW', \"bits_per_sample\": 8}, \"8 bit mu-law\"),\n",
        "    ({\"format\": \"gsm\"}, \"GSM-FR\"),\n",
        "    ({\"format\": \"mp3\", \"compression\": -9}, \"MP3\"),\n",
        "    ({\"format\": \"vorbis\", \"compression\": -1}, \"Vorbis\"),\n",
        "]\n",
        "for param, title in configs:\n",
        "  augmented = F.apply_codec(waveform, sample_rate, **param)\n",
        "  plot_specgram(augmented, sample_rate, title=title)\n",
        "  play_audio(augmented, sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIztfuydveC8"
      },
      "source": [
        "Simulating a phone recoding\n",
        "---------------------------\n",
        "\n",
        "Combining the previous techniques, we can simulate audio that sounds\n",
        "like a person talking over a phone in a echoey room with people talking\n",
        "in the background.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmBNQ1rQveC8"
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000\n",
        "speech, _ = get_speech_sample(resample=sample_rate)\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"Original\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# Apply RIR\n",
        "rir, _ = get_rir_sample(resample=sample_rate, processed=True)\n",
        "speech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\n",
        "speech = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"RIR Applied\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# Add background noise\n",
        "# Because the noise is recorded in the actual environment, we consider that\n",
        "# the noise contains the acoustic feature of the environment. Therefore, we add\n",
        "# the noise after RIR application.\n",
        "noise, _ = get_noise_sample(resample=sample_rate)\n",
        "noise = noise[:, :speech.shape[1]]\n",
        "\n",
        "snr_db = 8\n",
        "scale = math.exp(snr_db / 10) * noise.norm(p=2) / speech.norm(p=2)\n",
        "speech = (scale * speech + noise) / 2\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"BG noise added\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# Apply filtering and change sample rate\n",
        "speech, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
        "  speech,\n",
        "  sample_rate,\n",
        "  effects=[\n",
        "      [\"lowpass\", \"4000\"],\n",
        "      [\"compand\", \"0.02,0.05\", \"-60,-60,-30,-10,-20,-8,-5,-8,-2,-8\", \"-8\", \"-7\", \"0.05\"],\n",
        "      [\"rate\", \"8000\"],\n",
        "  ],\n",
        ")\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"Filtered\")\n",
        "play_audio(speech, sample_rate)\n",
        "\n",
        "# Apply telephony codec\n",
        "speech = F.apply_codec(speech, sample_rate, format=\"gsm\")\n",
        "\n",
        "plot_specgram(speech, sample_rate, title=\"GSM Codec Applied\")\n",
        "play_audio(speech, sample_rate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "audio_data_augmentation_tutorial.ipynb のコピー",
      "provenance": [
        {
          "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/ef328965f94a4ffb46e5b6972e26cadc/audio_data_augmentation_tutorial.ipynb",
          "timestamp": 1653888067016
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
